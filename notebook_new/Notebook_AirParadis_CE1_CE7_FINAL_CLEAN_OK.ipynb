{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aecf57c",
   "metadata": {},
   "source": [
    "# üß† Notebook complet - Approches avanc√©es Air Paradis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b062c",
   "metadata": {},
   "source": [
    "Ce notebook couvre les pr√©traitements, embeddings, mod√®les Keras (dense, LSTM), BERT et Universal Sentence Encoder selon les crit√®res CE1 √† CE7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cd9aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- [1. Import du jeu de donn√©es et v√©rification de l'√©quilibrage](#1.-Import-du-jeu-de-donn√©es-et-v√©rification-de-l'√©quilibrage)\n",
       "- [2. Pr√©traitement du texte (nettoyage + stemming/lemmatisation)](#2.-Pr√©traitement-du-texte-(nettoyage-+-stemming/lemmatisation))\n",
       "- [3.1 Baseline : TF-IDF + R√©gression Logistique](#3.1-Baseline-:-TF-IDF-+-R√©gression-Logistique)\n",
       "- [3.2 Baseline : LSTM (Keras)](#3.2-Baseline-:-LSTM-(Keras))\n",
       "- [4. Embeddings Word2Vec, GloVe, FastText](#4.-Embeddings-Word2Vec,-GloVe,-FastText)\n",
       "  - [4.1 Chargement des embeddings GloVe et FastText](#4.1-Chargement-des-embeddings-GloVe-et-FastText)\n",
       "- [5. Mod√®les Keras avec embeddings](#5.-Mod√®les-Keras-avec-embeddings)\n",
       "  - [5.0 Int√©gration GloVe et FastText dans Keras](#5.0-Int√©gration-GloVe-et-FastText-dans-Keras)\n",
       "  - [5.1 Mod√®le Keras avec Word2Vec](#5.1-Mod√®le-Keras-avec-Word2Vec)\n",
       "  - [5.2 Mod√®le Keras avec GloVe](#5.2-Mod√®le-Keras-avec-GloVe)\n",
       "  - [5.3 Mod√®le Keras avec FastText](#5.3-Mod√®le-Keras-avec-FastText)\n",
       "- [6. BERT - Fine-tuning HuggingFace](#6.-BERT---Fine-tuning-HuggingFace)\n",
       "- [7. Universal Sentence Encoder](#7.-Universal-Sentence-Encoder)\n",
       "- [8. Synth√®se comparative des mod√®les](#8.-Synth√®se-comparative-des-mod√®les)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìö Table des mati√®res automatique\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown('''\n",
    "- [1. Import du jeu de donn√©es et v√©rification de l'√©quilibrage](#1.-Import-du-jeu-de-donn√©es-et-v√©rification-de-l'√©quilibrage)\n",
    "- [2. Pr√©traitement du texte (nettoyage + stemming/lemmatisation)](#2.-Pr√©traitement-du-texte-(nettoyage-+-stemming/lemmatisation))\n",
    "- [3.1 Baseline : TF-IDF + R√©gression Logistique](#3.1-Baseline-:-TF-IDF-+-R√©gression-Logistique)\n",
    "- [3.2 Baseline : LSTM (Keras)](#3.2-Baseline-:-LSTM-(Keras))\n",
    "- [4. Embeddings Word2Vec, GloVe, FastText](#4.-Embeddings-Word2Vec,-GloVe,-FastText)\n",
    "  - [4.1 Chargement des embeddings GloVe et FastText](#4.1-Chargement-des-embeddings-GloVe-et-FastText)\n",
    "- [5. Mod√®les Keras avec embeddings](#5.-Mod√®les-Keras-avec-embeddings)\n",
    "  - [5.0 Int√©gration GloVe et FastText dans Keras](#5.0-Int√©gration-GloVe-et-FastText-dans-Keras)\n",
    "  - [5.1 Mod√®le Keras avec Word2Vec](#5.1-Mod√®le-Keras-avec-Word2Vec)\n",
    "  - [5.2 Mod√®le Keras avec GloVe](#5.2-Mod√®le-Keras-avec-GloVe)\n",
    "  - [5.3 Mod√®le Keras avec FastText](#5.3-Mod√®le-Keras-avec-FastText)\n",
    "- [6. BERT - Fine-tuning HuggingFace](#6.-BERT---Fine-tuning-HuggingFace)\n",
    "- [7. Universal Sentence Encoder](#7.-Universal-Sentence-Encoder)\n",
    "- [8. Synth√®se comparative des mod√®les](#8.-Synth√®se-comparative-des-mod√®les)\n",
    "'''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc919a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/24 09:19:09 INFO mlflow.tracking.fluent: Experiment with name 'sentiment_analysis' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/750608397458522352', creation_time=1745500749339, experiment_id='750608397458522352', last_update_time=1745500749339, lifecycle_stage='active', name='sentiment_analysis', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"sentiment_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b9a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Version TF : 2.18.0\n",
      "üöÄ GPU d√©tect√© : []\n",
      "\n",
      "üîç Fichier cuDNN (cudnn64_8.dll) pr√©sent ?\n",
      "‚û°Ô∏è True\n",
      "\n",
      "üì¶ Tentative de chargement de la DLL cuDNN...\n",
      "‚úÖ cuDNN charg√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import ctypes\n",
    "# from pathlib import Path\n",
    "\n",
    "# print(\"üß† Version TF :\", tf.__version__)\n",
    "# print(\"üöÄ GPU d√©tect√© :\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# # V√©rification du fichier cuDNN\n",
    "# cudnn_path = \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2/bin/cudnn64_8.dll\"\n",
    "# print(\"\\nüîç Fichier cuDNN (cudnn64_8.dll) pr√©sent ?\")\n",
    "# print(\"‚û°Ô∏è\", Path(cudnn_path).exists())\n",
    "\n",
    "# print(\"\\nüì¶ Tentative de chargement de la DLL cuDNN...\")\n",
    "\n",
    "# try:\n",
    "#     ctypes.WinDLL(cudnn_path)\n",
    "#     print(\"‚úÖ cuDNN charg√© avec succ√®s !\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ùå Erreur lors du chargement de cuDNN :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe0f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üìä √âvaluation du mod√®le avec m√©triques + efficacit√©\n",
    "# # üîç Fonction d'√©valuation √©tendue : performances + l√©g√®ret√©\n",
    "# import tempfile\n",
    "# import time\n",
    "# import joblib\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report\n",
    "# from tensorflow.keras.models import save_model\n",
    "\n",
    "# def eval_model_extended(model, X_val, y_val, model_name=\"Mod√®le\", keras=False):\n",
    "#     print(f\"\\nüìå √âvaluation du mod√®le : {model_name}\")\n",
    "#     print(\"-------------------------------------\")\n",
    "\n",
    "#     # Inference & timing\n",
    "#     start = time.time()\n",
    "#     y_pred = model.predict(X_val)\n",
    "#     # elapsed = (time.time() - start) / len(X_val) * 1000  # ms/tweet\n",
    "#     elapsed = (time.time() - start) / X_val.shape[0] * 1000  # ms/tweet\n",
    "\n",
    "#     y_pred_label = (y_pred > 0.5).astype(int)\n",
    "#     acc = accuracy_score(y_val, y_pred_label)\n",
    "#     f1 = f1_score(y_val, y_pred_label)\n",
    "#     rec = recall_score(y_val, y_pred_label)\n",
    "\n",
    "#     print(f\"‚úÖ Accuracy : {acc:.4f}\")\n",
    "#     print(f\"‚úÖ F1-score : {f1:.4f}\")\n",
    "#     print(f\"‚úÖ Recall : {rec:.4f}\")\n",
    "\n",
    "#     # Taille du mod√®le\n",
    "#     tmp_path = tempfile.mktemp(suffix=\".h5\")\n",
    "#     # tmp_path = tempfile.mktemp()\n",
    "#     if keras:\n",
    "#         # save_model(model, tmp_path, save_format='h5')\n",
    "#         save_model(model, tmp_path + \".h5\")\n",
    "#     else:\n",
    "#         joblib.dump(model, tmp_path)\n",
    "\n",
    "#     # Attendre que le fichier existe avant de le mesurer\n",
    "#     if os.path.exists(tmp_path):    \n",
    "#         size_mb = os.path.getsize(tmp_path) / 1024 / 1024\n",
    "#         os.remove(tmp_path)  # Supprimer le fichier temporaire apr√®s la mesure\n",
    "#         print(f\"üíæ Taille du mod√®le : {size_mb:.2f} Mo\")\n",
    "#         print(f\"‚ö° Temps d'inf√©rence moyen : {elapsed:.2f} ms / tweet\")\n",
    "#     else:\n",
    "#         print(\"‚ùó Le fichier de mod√®le n‚Äôa pas √©t√© g√©n√©r√©. Impossible de mesurer sa taille.\")\n",
    "        \n",
    "#     # Nombre de param√®tres\n",
    "#     try:\n",
    "#         if keras:\n",
    "#             print(f\"üß† Nb param√®tres : {model.count_params()}\")\n",
    "#         elif hasattr(model, 'num_parameters'):\n",
    "#             print(f\"üß† Nb param√®tres : {model.num_parameters()}\")\n",
    "#     except:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8184a",
   "metadata": {},
   "source": [
    "## 1. Import du jeu de donn√©es et v√©rification de l'√©quilibrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6ac9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          0\n",
       "1  is upset that he can't update his Facebook by ...          0\n",
       "2  @Kenichan I dived many times for the ball. Man...          0\n",
       "3    my whole body feels itchy and like its on fire           0\n",
       "4  @nationwideclass no, it's not behaving at all....          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üì• Chargement du fichier CSV d'origine\n",
    "# Chargement du fichier CSV d'origine fourni (colonnes : target, id, date, flag, user, text)\n",
    "import pandas as pd\n",
    "\n",
    "input_path = 'C:/Users/sandr/OneDrive/Documents/JOB/OPENCLASSROOMS/AI_ENGINEER/Projet_7_R√©alisez_une_analyse_de_sentiments_gr√¢ce_au_Deep_Learning/Workspace/'\n",
    "\n",
    "raw_df = pd.read_csv(input_path + \"data/raw/training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", header=None)\n",
    "\n",
    "raw_df.columns = [\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "raw_df['sentiment'] = raw_df['sentiment'].replace({0: 0, 4: 1})  # 0: n√©gatif, 4: positif\n",
    "\n",
    "# On extrait uniquement le texte et le label\n",
    "df = raw_df[[\"text\", \"sentiment\"]].copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1d864f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©partition (%):\n",
      "sentiment\n",
      "0    50.0\n",
      "1    50.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %pip install seaborn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(data=df, x='sentiment')\n",
    "plt.title(\"R√©partition des tweets par sentiment\")\n",
    "plt.xticks([0,1], ['N√©gatif', 'Positif'])\n",
    "plt.show()\n",
    "\n",
    "counts = df['sentiment'].value_counts(normalize=True)\n",
    "print(\"R√©partition (%):\")\n",
    "print(counts * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d3e81",
   "metadata": {},
   "source": [
    "## 3. Echantillonnage stratifi√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1376df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âchantillon stratifi√© : 1000 tweets\n"
     ]
    }
   ],
   "source": [
    "# # üö¶ D√©tection RAM et d√©finition de SAMPLE_SIZE\n",
    "# import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# total_ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "# if total_ram_gb < 4:\n",
    "#     SAMPLE_SIZE = 5000\n",
    "# elif total_ram_gb < 8:\n",
    "#     SAMPLE_SIZE = 10000\n",
    "# elif total_ram_gb < 12:\n",
    "#     SAMPLE_SIZE = 20000\n",
    "# else:\n",
    "#     SAMPLE_SIZE = 30000\n",
    "\n",
    "# print(f\"üí° RAM d√©tect√©e : {total_ram_gb:.1f} Go - SAMPLE_SIZE s√©lectionn√© : {SAMPLE_SIZE}\")\n",
    "\n",
    "SAMPLE_SIZE = 1000  # Pour tests rapides\n",
    "\n",
    "# üéØ √âchantillonnage stratifi√© unique (commune √† tous les mod√®les)\n",
    "df_sample, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=SAMPLE_SIZE,\n",
    "    stratify=df['sentiment'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"√âchantillon stratifi√© : {df_sample.shape[0]} tweets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a354bf",
   "metadata": {},
   "source": [
    "V√©rifions l'√©quilibre de l'√©chantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883bc71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©partition (%):\n",
      "sentiment\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sns.countplot(data=df_sample, x='sentiment')\n",
    "plt.title(\"R√©partition des tweets par sentiment\")\n",
    "plt.xticks([0,1], ['N√©gatif', 'Positif'])\n",
    "plt.show()\n",
    "\n",
    "counts = df_sample['sentiment'].value_counts(normalize=True)\n",
    "print(\"R√©partition (%):\")\n",
    "print(counts * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c04c5d",
   "metadata": {},
   "source": [
    "L'√©chantillon est effectivement √©quilibr√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9e26e",
   "metadata": {},
   "source": [
    "## 2. Pr√©traitement du texte (nettoyage + stemming/lemmatisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947415b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# üßπ Nettoyage des tweets avec stemming et lemmatisation\n",
    "# Nettoyage des tweets √† partir du fichier brut\n",
    "# %pip install nltk\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, method='lemma'):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r\"@\\w+|#\", '', text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text.lower())\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    if method == 'stem':\n",
    "        return ' '.join([stemmer.stem(w) for w in words])\n",
    "    else:\n",
    "        return ' '.join([lemmatizer.lemmatize(w) for w in words])\n",
    "\n",
    "# Application sur le jeu de donn√©es brut\n",
    "df_sample['text_stem'] = df_sample['text'].apply(lambda x: clean_text(x, 'stem'))\n",
    "df_sample['text_lemma'] = df_sample['text'].apply(lambda x: clean_text(x, 'lemma'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f235ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Initialisation de MLflow enrichie pour suivi des performances et efficacit√©\n",
    "# import mlflow\n",
    "import mlflow.sklearn\n",
    "import time\n",
    "import os\n",
    "\n",
    "mlflow.set_experiment(\"air_paradis_sentiment\")\n",
    "\n",
    "def log_model_metrics(model, X_val, y_val, model_name, keras=False):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        start = time.time()\n",
    "        y_pred = model.predict(X_val)\n",
    "        elapsed = (time.time() - start) / X_val.shape[0] * 1000  # ms/sample\n",
    "\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "        acc = accuracy_score(y_val, y_pred_label)\n",
    "        f1 = f1_score(y_val, y_pred_label)\n",
    "        rec = recall_score(y_val, y_pred_label)\n",
    "\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"recall\", rec)\n",
    "        mlflow.log_metric(\"inference_time_ms\", elapsed)\n",
    "\n",
    "        import tempfile\n",
    "        if keras:\n",
    "            from tensorflow.keras.models import save_model\n",
    "            model_path = tempfile.mktemp(suffix=\".h5\")\n",
    "            save_model(model, model_path)\n",
    "        else:\n",
    "            import joblib\n",
    "            model_path = tempfile.mktemp(suffix=\".pkl\")\n",
    "            joblib.dump(model, model_path)\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            size_mb = os.path.getsize(model_path) / 1024 / 1024\n",
    "            mlflow.log_metric(\"model_size_mb\", size_mb)\n",
    "            mlflow.log_artifact(model_path, artifact_path=\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873c2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def eval_model_extended(model, X_val, y_val, model_name=\"Model\", keras=False, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"\\nüìå √âvaluation du mod√®le : {model_name}\")\n",
    "        print(\"-\" * 37)\n",
    "\n",
    "    # ‚öôÔ∏è Pr√©diction\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # üîÅ Conversion si n√©cessaire\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.ravel()\n",
    "    if keras:\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # üìä M√©triques\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Accuracy : {acc:.4f}\")\n",
    "        print(f\"‚úÖ F1-score : {f1:.4f}\")\n",
    "        print(f\"‚úÖ Recall : {recall:.4f}\")\n",
    "\n",
    "    # üíæ Taille du mod√®le\n",
    "    try:\n",
    "        tmp_path = tempfile.mktemp(suffix=\".keras\" if keras else \".pkl\")\n",
    "        if keras:\n",
    "            model.save(tmp_path)  # Plus de `save_format` en Keras 3+\n",
    "        else:\n",
    "            import joblib\n",
    "            joblib.dump(model, tmp_path)\n",
    "\n",
    "        size_mb = os.path.getsize(tmp_path) / 1024 / 1024\n",
    "        os.remove(tmp_path)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"üíæ Taille du mod√®le : {size_mb:.2f} Mo\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(\"‚ùó Le fichier de mod√®le n‚Äôa pas √©t√© g√©n√©r√©. Impossible de mesurer sa taille.\")\n",
    "            print(f\"  Raison : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39b6b3",
   "metadata": {},
   "source": [
    "## 3.1 Baseline : TF-IDF + R√©gression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1115694",
   "metadata": {},
   "source": [
    "Param√©trer MLFlow pour stocker les r√©sultats des tests  !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c226ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå √âvaluation du mod√®le : TF-IDF + LogReg\n",
      "-------------------------------------\n",
      "‚úÖ Accuracy : 0.7067\n",
      "‚úÖ F1-score : 0.7105\n",
      "‚úÖ Recall : 0.7200\n",
      "üíæ Taille du mod√®le : 0.02 Mo\n"
     ]
    }
   ],
   "source": [
    "# üìä TF-IDF + R√©gression Logistique\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df_sample['text_lemma'])\n",
    "\n",
    "# S√©paration\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_vec, X_temp_vec, y_train, y_temp = train_test_split(X_tfidf, df_sample['sentiment'], test_size=0.3, stratify=df_sample['sentiment'], random_state=42)\n",
    "X_val_vec, X_test_vec, y_val, y_test = train_test_split(X_temp_vec, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Entra√Ænement\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# √âvaluation\n",
    "eval_model_extended(model, X_val_vec, y_val, \"TF-IDF + LogReg\", keras=False)\n",
    "\n",
    "log_model_metrics(model, X_val_vec, y_val, \"TF-IDF + LogReg\", keras=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15325391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # üî¢ Param√®tres\n",
    "# # SAMPLE_SIZE = 10000  # √† ajuster selon ta RAM\n",
    "# MAX_FEATURES = 5000\n",
    "\n",
    "# # üéØ √âchantillonnage stratifi√©\n",
    "# # df_sample, _ = train_test_split(\n",
    "# #     df,\n",
    "# #     train_size=SAMPLE_SIZE,\n",
    "# #     stratify=df['sentiment'],\n",
    "# #     random_state=42\n",
    "# # )\n",
    "\n",
    "# # üß† Vectorisation TF-IDF\n",
    "# vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "# X_tfidf = vectorizer.fit_transform(df_sample['text_lemma'])\n",
    "# y = df_sample['sentiment']\n",
    "\n",
    "# # ‚úÇÔ∏è S√©paration train / val / test stratifi√©e\n",
    "# X_train_vec, X_temp_vec, y_train, y_temp = train_test_split(\n",
    "#     X_tfidf, y, test_size=0.3, stratify=y, random_state=42\n",
    "# )\n",
    "# X_val_vec, X_test_vec, y_val, y_test = train_test_split(\n",
    "#     X_temp_vec, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    "# )\n",
    "\n",
    "# # üöÄ Entra√Ænement du mod√®le\n",
    "# model = LogisticRegression(max_iter=200)\n",
    "# model.fit(X_train_vec, y_train)\n",
    "\n",
    "# # üß™ √âvaluation\n",
    "# eval_model_extended(model, X_val_vec, y_val, \"TF-IDF + LogReg\", keras=False)\n",
    "# log_model_metrics(model, X_val_vec, y_val, \"TF-IDF + LogReg\", keras=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc76f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a2f598",
   "metadata": {},
   "source": [
    "## 3.2 Baseline : LSTM (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b493eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "6/6 [==============================] - 5s 236ms/step - loss: 0.6932 - accuracy: 0.5143 - val_loss: 0.6926 - val_accuracy: 0.5333\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 5s 236ms/step - loss: 0.6932 - accuracy: 0.5143 - val_loss: 0.6926 - val_accuracy: 0.5333\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6885 - accuracy: 0.6514 - val_loss: 0.6912 - val_accuracy: 0.5067\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6885 - accuracy: 0.6514 - val_loss: 0.6912 - val_accuracy: 0.5067\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.6794 - accuracy: 0.6757 - val_loss: 0.6887 - val_accuracy: 0.5333\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.6794 - accuracy: 0.6757 - val_loss: 0.6887 - val_accuracy: 0.5333\n",
      "\n",
      "üìå √âvaluation du mod√®le : LSTM Baseline\n",
      "-------------------------------------\n",
      "\n",
      "üìå √âvaluation du mod√®le : LSTM Baseline\n",
      "-------------------------------------\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "‚úÖ Accuracy : 0.5333\n",
      "‚úÖ F1-score : 0.6392\n",
      "‚úÖ Recall : 0.8267\n",
      "‚úÖ Accuracy : 0.5333\n",
      "‚úÖ F1-score : 0.6392\n",
      "‚úÖ Recall : 0.8267\n",
      "üíæ Taille du mod√®le : 15.05 Moüíæ Taille du mod√®le : 15.05 Mo\n",
      "1/5 [=====>........................] - ETA: 0s\n",
      "5/5 [==============================] - 0s 10ms/step\n",
      "5/5 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_21284\\1609296360.py:34: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "# üìä LSTM Baseline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pr√©paration des donn√©es\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_sample['text_lemma'])\n",
    "X_seq = tokenizer.texts_to_sequences(df_sample['text_lemma'])\n",
    "X_pad = pad_sequences(X_seq, maxlen=50)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_pad, df_sample['sentiment'], test_size=0.3, stratify=df_sample['sentiment'], random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Mod√®le LSTM\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=64, input_length=50),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "# √âvaluation\n",
    "eval_model_extended(model_lstm, X_val, y_val, \"LSTM Baseline\", keras=True)\n",
    "\n",
    "log_model_metrics(model_lstm, X_val, y_val, \"LSTM Baseline\", keras=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4394eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import psutil\n",
    "\n",
    "# # üö¶ √âtape 1 : d√©tection de la RAM et SAMPLE_SIZE\n",
    "# total_ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "\n",
    "# if total_ram_gb < 4:\n",
    "#     SAMPLE_SIZE = 5000\n",
    "# elif total_ram_gb < 8:\n",
    "#     SAMPLE_SIZE = 10000\n",
    "# elif total_ram_gb < 12:\n",
    "#     SAMPLE_SIZE = 20000\n",
    "# else:\n",
    "#     SAMPLE_SIZE = 30000\n",
    "\n",
    "# print(f\"üí° RAM d√©tect√©e : {total_ram_gb:.1f} Go - SAMPLE_SIZE s√©lectionn√© : {SAMPLE_SIZE}\")\n",
    "\n",
    "# üî¢ Param√®tres\n",
    "# SAMPLE_SIZE = 10000  # √† ajuster selon ta RAM\n",
    "# MAX_FEATURES = 5000\n",
    "\n",
    "# üéØ √âtape 2 : √©chantillonnage stratifi√©\n",
    "# df_sample_lstm, _ = train_test_split(\n",
    "#     df,\n",
    "#     train_size=SAMPLE_SIZE,\n",
    "#     stratify=df['sentiment'],\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # üî§ √âtape 3 : pr√©paration des s√©quences\n",
    "# tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(df_sample['text_lemma'])\n",
    "\n",
    "# X_seq = tokenizer.texts_to_sequences(df_sample['text_lemma'])\n",
    "# X_pad = pad_sequences(X_seq, maxlen=50)\n",
    "# # y = df_sample_lstm['sentiment']\n",
    "# y = df_sample['sentiment']\n",
    "\n",
    "# # ‚úÇÔ∏è √âtape 4 : split train / val / test\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#     X_pad, y, test_size=0.3, stratify=y, random_state=42\n",
    "# )\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    "# )\n",
    "\n",
    "# # üß† √âtape 5 : Mod√®le LSTM\n",
    "# model_lstm = Sequential([\n",
    "#     Embedding(input_dim=20000, output_dim=64, input_length=50),\n",
    "#     LSTM(64),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # üöÄ √âtape 6 : Entra√Ænement\n",
    "# model_lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "# # üß™ √âtape 7 : √âvaluation\n",
    "# eval_model_extended(model_lstm, X_val, y_val, \"LSTM Baseline\", keras=True)\n",
    "# log_model_metrics(model_lstm, X_val, y_val, \"LSTM Baseline\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3625e4c",
   "metadata": {},
   "source": [
    "LSTM tout seul = Fait\n",
    "LSTM + Word2Vec\n",
    "LSTM + Fastext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf663a",
   "metadata": {},
   "source": [
    "## 4. Embeddings Word2Vec, GloVe, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05bb744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Si X_train contient des vecteurs ‚Üí il faut retourner au texte\n",
    "# Exemple : on repart de la colonne text_lemma du DataFrame de base\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "X_train_idx, _, y_train, _ = train_test_split(df_sample.index, df_sample['sentiment'], test_size=0.3, stratify=df_sample['sentiment'], random_state=42)\n",
    "\n",
    "# Exemple : extraction du texte source\n",
    "X_train_text = df_sample.loc[X_train_idx, \"text_lemma\"]  # ou bien une version d√©j√† s√©par√©e\n",
    "\n",
    "sentences = [text.split() for text in X_train_text]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "w2v_model.save(input_path + \"models/word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266dda9",
   "metadata": {},
   "source": [
    "### 4.1 Chargement des embeddings GloVe et FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda81418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe charg√© : 400000 mots\n",
      "FastText charg√© : 999995 mots\n"
     ]
    }
   ],
   "source": [
    "# üìö Fonction utilitaire pour charger des embeddings externes (GloVe/FastText)\n",
    "import numpy as np\n",
    "\n",
    "def load_embedding(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Chemins vers les fichiers d'embedding\n",
    "glove_path = input_path + 'data/embeddings/glove.6B.100d.txt'\n",
    "fasttext_path = input_path + 'data/embeddings/wiki-news-300d-1M.vec'\n",
    "\n",
    "# Chargement\n",
    "glove_embeddings = load_embedding(glove_path)\n",
    "fasttext_embeddings = load_embedding(fasttext_path)\n",
    "\n",
    "print(f\"GloVe charg√© : {len(glove_embeddings)} mots\")\n",
    "print(f\"FastText charg√© : {len(fasttext_embeddings)} mots\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4257273",
   "metadata": {},
   "source": [
    "## 5. Mod√®les Keras avec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785ccc6",
   "metadata": {},
   "source": [
    "### 5.1 Mod√®le Keras avec Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227f93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 56ms/step - loss: 0.6936 - accuracy: 0.4900 - val_loss: 0.6931 - val_accuracy: 0.5067\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 1s 56ms/step - loss: 0.6936 - accuracy: 0.4900 - val_loss: 0.6931 - val_accuracy: 0.5067\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6926 - accuracy: 0.5400 - val_loss: 0.6930 - val_accuracy: 0.5333\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6926 - accuracy: 0.5400 - val_loss: 0.6930 - val_accuracy: 0.5333\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6921 - accuracy: 0.5771 - val_loss: 0.6928 - val_accuracy: 0.5267\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6921 - accuracy: 0.5771 - val_loss: 0.6928 - val_accuracy: 0.5267\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6916 - accuracy: 0.5571 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6916 - accuracy: 0.5571 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6903 - accuracy: 0.5729 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6903 - accuracy: 0.5729 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "\n",
      "üìå √âvaluation du mod√®le : Word2Vec\n",
      "-------------------------------------\n",
      "\n",
      "üìå √âvaluation du mod√®le : Word2Vec\n",
      "-------------------------------------\n",
      "5/5 [==============================] - 0s 288us/step\n",
      "5/5 [==============================] - 0s 288us/step\n",
      "‚úÖ Accuracy : 0.5000\n",
      "‚úÖ F1-score : 0.0000\n",
      "‚úÖ Recall : 0.0000\n",
      "‚úÖ Accuracy : 0.5000\n",
      "‚úÖ F1-score : 0.0000\n",
      "‚úÖ Recall : 0.0000\n",
      "üíæ Taille du mod√®le : 14.72 Mo\n",
      "üíæ Taille du mod√®le : 14.72 Mo\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_21284\\1609296360.py:34: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "# üß† Construction du mod√®le Keras avec couche d'embedding\n",
    "# üìè Padding des s√©quences textuelles √† longueur fixe\n",
    "# üî¢ Tokenisation des textes pour entra√Ænement LSTM ou Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = df_sample['text_lemma']\n",
    "y = df_sample['sentiment']\n",
    "\n",
    "X_train_text, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val_text, X_test_text, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=50)\n",
    "X_val_pad = pad_sequences(tokenizer.texts_to_sequences(X_val_text), maxlen=50)\n",
    "\n",
    "model_w2v = Sequential([\n",
    "    Embedding(20000, 64, input_length=50),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_w2v.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_w2v.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=5, batch_size=128)\n",
    "\n",
    "eval_model_extended(model_w2v, X_val_pad, y_val, \"Word2Vec\", keras=True)\n",
    "log_model_metrics(model_w2v, X_val_pad, y_val, \"Word2Vec\", keras=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f88aca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Initialisation du tokenizer\n",
    "# tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(X_train_text)  # X_train_text contient le texte brut\n",
    "\n",
    "# # Tokenisation + padding\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "# X_val_seq = tokenizer.texts_to_sequences(X_val_text)\n",
    "\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=50)\n",
    "# X_val_pad = pad_sequences(X_val_seq, maxlen=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c22f5",
   "metadata": {},
   "source": [
    "### 5.5 LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74733246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 3s 154ms/step - loss: 0.6935 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 3s 154ms/step - loss: 0.6935 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "\n",
      "üìå √âvaluation du mod√®le : LSTM + Word2Vec\n",
      "-------------------------------------\n",
      "\n",
      "üìå √âvaluation du mod√®le : LSTM + Word2Vec\n",
      "-------------------------------------\n",
      "5/5 [==============================] - 1s 7ms/step\n",
      "5/5 [==============================] - 1s 7ms/step\n",
      "‚úÖ Accuracy : 0.5000\n",
      "‚úÖ F1-score : 0.0000\n",
      "‚úÖ Recall : 0.0000\n",
      "üíæ Taille du mod√®le : 1.38 Mo\n",
      "‚úÖ Accuracy : 0.5000\n",
      "‚úÖ F1-score : 0.0000\n",
      "‚úÖ Recall : 0.0000\n",
      "üíæ Taille du mod√®le : 1.38 Mo\n",
      "5/5 [==============================] - 0s 11ms/step\n",
      "5/5 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_21284\\1609296360.py:34: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "# üìä Mod√®le LSTM avec Word2Vec\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "model_lstm_w2v = Sequential([\n",
    "    Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "              weights=[embedding_matrix], input_length=X_train_pad.shape[1], trainable=False),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_lstm_w2v.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm_w2v.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "eval_model_extended(model_lstm_w2v, X_val_pad, y_val, \"LSTM + Word2Vec\", keras=True)\n",
    "log_model_metrics(model_lstm_w2v, X_val_pad, y_val, \"LSTM + Word2Vec\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bf1a6",
   "metadata": {},
   "source": [
    "### 5.6 LSTM + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1c51355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrainement du mod√®le FastText\n",
    "# from gensim.models import FastText\n",
    "\n",
    "# sentences = [text.split() for text in X_train_text]  # liste de listes de mots\n",
    "# ft_model = FastText(sentences, vector_size=100, window=5, min_count=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a1dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üìä Mod√®le LSTM avec FastText\n",
    "# embedding_dim = 300\n",
    "# embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     if word in ft_model:\n",
    "#         embedding_matrix[i] = ft_model[word]\n",
    "\n",
    "# model_lstm_ft = Sequential([\n",
    "#     Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "#               weights=[embedding_matrix], input_length=X_train_pad.shape[1], trainable=False),\n",
    "#     LSTM(64),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model_lstm_ft.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model_lstm_ft.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "# eval_model_extended(model_lstm_ft, X_val_pad, y_val, \"LSTM + FastText\", keras=True)\n",
    "# log_model_metrics(model_lstm_ft, X_val_pad, y_val, \"LSTM + FastText\", keras=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9869df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 3s 219ms/step - loss: 0.6934 - accuracy: 0.4771 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 3s 219ms/step - loss: 0.6934 - accuracy: 0.4771 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "\n",
      "üìå √âvaluation du mod√®le : LSTM + FastText (local)\n",
      "-------------------------------------\n",
      "\n",
      "üìå √âvaluation du mod√®le : LSTM + FastText (local)\n",
      "-------------------------------------\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "‚úÖ Accuracy : 0.5000\n",
      "‚úÖ F1-score : 0.0000\n",
      "‚úÖ Recall : 0.0000\n",
      "üíæ Taille du mod√®le : 1.38 Mo\n",
      "‚úÖ Accuracy : 0.5000\n",
      "‚úÖ F1-score : 0.0000\n",
      "‚úÖ Recall : 0.0000\n",
      "üíæ Taille du mod√®le : 1.38 Mo\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_21284\\1609296360.py:34: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "# üìä Mod√®le LSTM avec FastText (entra√Ænement local)\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "\n",
    "# 1. Entra√Ænement du mod√®le FastText sur les donn√©es d'entra√Ænement\n",
    "sentences = [text.split() for text in X_train_text]\n",
    "ft_model = FastText(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 2. Cr√©ation de l'embedding matrix √† partir du tokenizer\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in ft_model.wv:\n",
    "        embedding_matrix[i] = ft_model.wv[word]\n",
    "\n",
    "# 3. Cr√©ation du mod√®le LSTM avec embedding FastText\n",
    "model_lstm_ft = Sequential([\n",
    "    Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "              weights=[embedding_matrix], input_length=X_train_pad.shape[1], trainable=False),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_lstm_ft.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm_ft.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "# 4. √âvaluation et suivi MLflow\n",
    "eval_model_extended(model_lstm_ft, X_val_pad, y_val, \"LSTM + FastText (local)\", keras=True)\n",
    "log_model_metrics(model_lstm_ft, X_val_pad, y_val, \"LSTM + FastText (local)\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481be718",
   "metadata": {},
   "source": [
    "### 5.0 Int√©gration GloVe et FastText dans Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b21b757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Fonction utilitaire pour charger des embeddings externes (GloVe/FastText)\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour charger des embeddings pr√©-entra√Æn√©s\n",
    "def load_embedding(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Charger GloVe\n",
    "glove_path = input_path + 'data/embeddings/glove.6B.100d.txt'\n",
    "glove_embeddings = load_embedding(glove_path)\n",
    "\n",
    "# Charger FastText\n",
    "fasttext_path = input_path + 'data/embeddings/wiki-news-300d-1M.vec'\n",
    "fasttext_embeddings = load_embedding(fasttext_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4394d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les matrices d'embedding pour notre vocabulaire\n",
    "embedding_dim_glove = 100\n",
    "embedding_dim_fasttext = 300\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix_glove = np.zeros((20000, embedding_dim_glove))\n",
    "embedding_matrix_fasttext = np.zeros((20000, embedding_dim_fasttext))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= 20000:\n",
    "        continue\n",
    "    vec_g = glove_embeddings.get(word)\n",
    "    if vec_g is not None:\n",
    "        embedding_matrix_glove[i] = vec_g\n",
    "    vec_f = fasttext_embeddings.get(word)\n",
    "    if vec_f is not None:\n",
    "        embedding_matrix_fasttext[i] = vec_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544e641",
   "metadata": {},
   "source": [
    "### 5.2 Mod√®le Keras avec GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "364868b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 4s 196ms/step - loss: 0.6897 - accuracy: 0.5329 - val_loss: 0.6934 - val_accuracy: 0.4867\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 4s 196ms/step - loss: 0.6897 - accuracy: 0.5329 - val_loss: 0.6934 - val_accuracy: 0.4867\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6733 - accuracy: 0.5900 - val_loss: 0.6925 - val_accuracy: 0.5400\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6733 - accuracy: 0.5900 - val_loss: 0.6925 - val_accuracy: 0.5400\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6612 - accuracy: 0.6329 - val_loss: 0.6905 - val_accuracy: 0.5467\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6612 - accuracy: 0.6329 - val_loss: 0.6905 - val_accuracy: 0.5467\n",
      "\n",
      "üìå √âvaluation du mod√®le : GloVe\n",
      "-------------------------------------\n",
      "\n",
      "üìå √âvaluation du mod√®le : GloVe\n",
      "-------------------------------------\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "‚úÖ Accuracy : 0.5467\n",
      "‚úÖ F1-score : 0.5584\n",
      "‚úÖ Recall : 0.5733\n",
      "üíæ Taille du mod√®le : 8.14 Mo\n",
      "‚úÖ Accuracy : 0.5467\n",
      "‚úÖ F1-score : 0.5584\n",
      "‚úÖ Recall : 0.5733\n",
      "üíæ Taille du mod√®le : 8.14 Mo\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_21284\\1609296360.py:34: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "# üìä Mod√®le Keras avec GloVe\n",
    "model_glove = Sequential([\n",
    "    Embedding(input_dim=embedding_matrix_glove.shape[0], output_dim=embedding_matrix_glove.shape[1],\n",
    "              weights=[embedding_matrix_glove], input_length=X_pad.shape[1], trainable=False),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_glove.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "# √âvaluation\n",
    "eval_model_extended(model_glove, X_val, y_val, \"GloVe\", keras=True)\n",
    "\n",
    "log_model_metrics(model_glove, X_val, y_val, \"GloVe\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909827e",
   "metadata": {},
   "source": [
    "### 5.3 Mod√®le Keras avec FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f886642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 5s 240ms/step - loss: 0.6946 - accuracy: 0.5043 - val_loss: 0.6930 - val_accuracy: 0.5133\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 5s 240ms/step - loss: 0.6946 - accuracy: 0.5043 - val_loss: 0.6930 - val_accuracy: 0.5133\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6861 - accuracy: 0.5571 - val_loss: 0.6913 - val_accuracy: 0.5400\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6861 - accuracy: 0.5571 - val_loss: 0.6913 - val_accuracy: 0.5400\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6799 - accuracy: 0.6086 - val_loss: 0.6896 - val_accuracy: 0.5267\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6799 - accuracy: 0.6086 - val_loss: 0.6896 - val_accuracy: 0.5267\n",
      "\n",
      "üìå √âvaluation du mod√®le : FastText\n",
      "-------------------------------------\n",
      "\n",
      "üìå √âvaluation du mod√®le : FastText\n",
      "-------------------------------------\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "5/5 [==============================] - 1s 8ms/step\n",
      "‚úÖ Accuracy : 0.5267\n",
      "‚úÖ F1-score : 0.4818\n",
      "‚úÖ Recall : 0.4400\n",
      "‚úÖ Accuracy : 0.5267\n",
      "‚úÖ F1-score : 0.4818\n",
      "‚úÖ Recall : 0.4400\n",
      "üíæ Taille du mod√®le : 23.98 Mo\n",
      "1/5 [=====>........................] - ETA: 0süíæ Taille du mod√®le : 23.98 Mo\n",
      "5/5 [==============================] - 0s 12ms/step\n",
      "5/5 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp\\ipykernel_21284\\1609296360.py:34: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "# üìä Mod√®le Keras avec FastText\n",
    "model_fasttext = Sequential([\n",
    "    Embedding(input_dim=embedding_matrix_fasttext.shape[0], output_dim=embedding_matrix_fasttext.shape[1],\n",
    "              weights=[embedding_matrix_fasttext], input_length=X_pad.shape[1], trainable=False),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_fasttext.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_fasttext.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=128)\n",
    "\n",
    "# √âvaluation\n",
    "eval_model_extended(model_fasttext, X_val, y_val, \"FastText\", keras=True)\n",
    "\n",
    "log_model_metrics(model_fasttext, X_val, y_val, \"FastText\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab0aa6",
   "metadata": {},
   "source": [
    "BERT avec Embedding + classification !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9e5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f972d1f6",
   "metadata": {},
   "source": [
    "## 6. BERT - Fine-tuning HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbcebed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\sandr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\sandr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       100\n",
      "           1       0.75      0.77      0.76       100\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.76      0.76      0.76       200\n",
      "weighted avg       0.76      0.76      0.76       200\n",
      "\n",
      "\n",
      "üìå √âvaluation du mod√®le : BERT + LogisticRegression\n",
      "-------------------------------------\n",
      "‚úÖ Accuracy : 0.7600\n",
      "‚úÖ F1-score : 0.7624\n",
      "‚úÖ Recall : 0.7700\n",
      "üíæ Taille du mod√®le : 0.01 Mo\n"
     ]
    }
   ],
   "source": [
    "# üìä BERT + R√©gression Logistique (embedding)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Chargement du tokenizer et mod√®le BERT (non fine-tun√©)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, max_len=64):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # CLS token\n",
    "\n",
    "# Pour acc√©l√©rer la d√©mo, on prend un √©chantillon r√©duit (ex : 5000 tweets)\n",
    "# sample_df = df.sample(5000, random_state=42)\n",
    "# X_bert = get_bert_embeddings(sample_df['text_lemma'].tolist(), tokenizer, bert_model)\n",
    "# y_bert = sample_df['sentiment'].values\n",
    "\n",
    "X_bert = get_bert_embeddings(df_sample['text_lemma'].tolist(), tokenizer, bert_model)\n",
    "y_bert = df_sample['sentiment'].values\n",
    "\n",
    "# Split + Mod√®le\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bert, y_bert, test_size=0.2, stratify=y_bert, random_state=42)\n",
    "model_bert = LogisticRegression(max_iter=(100)) # au lieu de 1000\n",
    "model_bert.fit(X_train, y_train)\n",
    "y_pred = model_bert.predict(X_test)\n",
    "\n",
    "# √âvaluation\n",
    "# print(\"\\nüìå √âvaluation du mod√®le : BERT + LogisticRegression\")\n",
    "# print(\"----------------------------------------\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# üîç √âvaluation compl√®te avec l√©g√®ret√© et performance\n",
    "eval_model_extended(model_bert, X_test, y_test, model_name=\"BERT + LogisticRegression\", keras=False)\n",
    "\n",
    "log_model_metrics(model_bert, X_test, y_test, \"BERT + LogisticRegression\", keras=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d32bd39",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üìä BERT Embedding + Dense NN Classifier\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Rechargement BERT si n√©cessaire\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, max_len=64):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # vecteurs CLS\n",
    "\n",
    "# Extrait un sous-√©chantillon (sinon tr√®s long √† calculer)\n",
    "# sample_df = df.sample(1000, random_state=42)\n",
    "# X_bert_embed = get_bert_embeddings(sample_df['text_lemma'].tolist(), tokenizer, bert_model)\n",
    "# y_bert_embed = sample_df['sentiment'].values\n",
    "\n",
    "X_bert_embed = get_bert_embeddings(df_sample['text_lemma'].tolist(), tokenizer, bert_model)\n",
    "y_bert_embed = df_sample['sentiment'].values\n",
    "\n",
    "# Split jeu\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_bert_embed, y_bert_embed, test_size=0.2, stratify=y_bert_embed, random_state=42)\n",
    "\n",
    "# Mod√®le Keras simple\n",
    "model_bert_dense = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(768,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_bert_dense.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_bert_dense.fit(X_train_b, y_train_b, validation_data=(X_val_b, y_val_b), epochs=3, batch_size=128)\n",
    "\n",
    "# √âvaluation et log MLflow\n",
    "eval_model_extended(model_bert_dense, X_val_b, y_val_b, \"BERT + Dense\", keras=True)\n",
    "log_model_metrics(model_bert_dense, X_val_b, y_val_b, \"BERT + Dense\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2b849",
   "metadata": {},
   "source": [
    "### 6.3 TFBertModel (Hugging Face) + Classification (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7145f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sandr\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# √âchantillon\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# sample_df = df.sample(5000, random_state=42)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# X_embed = get_tfbert_embeddings(sample_df['text_lemma'].tolist(), tokenizer, bert_model_tf)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# y_embed = sample_df['sentiment'].values\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m X_embed \u001b[38;5;241m=\u001b[39m get_tfbert_embeddings(\u001b[43mdf_sample\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_lemma\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), tokenizer, bert_model_tf)\n\u001b[0;32m     24\u001b[0m y_embed \u001b[38;5;241m=\u001b[39m df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Split\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# üìä Embedding avec TFBertModel + classification Keras\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Chargement du mod√®le et tokenizer Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model_tf = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Pr√©paration des entr√©es BERT\n",
    "def get_tfbert_embeddings(texts, tokenizer, model, max_len=64):\n",
    "    inputs = tokenizer(texts, padding='max_length', truncation=True,\n",
    "                      return_tensors=\"tf\", max_length=max_len)\n",
    "    outputs = model(inputs)[0][:, 0, :]  # vecteur CLS\n",
    "    return outputs.numpy()\n",
    "\n",
    "# √âchantillon\n",
    "# sample_df = df.sample(5000, random_state=42)\n",
    "# X_embed = get_tfbert_embeddings(sample_df['text_lemma'].tolist(), tokenizer, bert_model_tf)\n",
    "# y_embed = sample_df['sentiment'].values\n",
    "\n",
    "X_embed = get_tfbert_embeddings(df_sample['text_lemma'].tolist(), tokenizer, bert_model_tf)\n",
    "y_embed = df_sample['sentiment'].values\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_embed, y_embed, test_size=0.2, stratify=y_embed, random_state=42)\n",
    "\n",
    "# Mod√®le Keras (classification)\n",
    "model_tfbert_dense = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(768,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_tfbert_dense.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_tfbert_dense.fit(X_train_b, y_train_b, validation_data=(X_val_b, y_val_b), epochs=3, batch_size=128)\n",
    "\n",
    "# √âvaluation et log\n",
    "eval_model_extended(model_tfbert_dense, X_val_b, y_val_b, \"TFBertModel + Dense\", keras=True)\n",
    "log_model_metrics(model_tfbert_dense, X_val_b, y_val_b, \"TFBertModel + Dense\", keras=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e3549",
   "metadata": {},
   "source": [
    "## 8. Synth√®se comparative des mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "979b68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üìä Synth√®se manuelle des performances √† compl√©ter apr√®s ex√©cution\n",
    "# import pandas as pd\n",
    "\n",
    "# summary_data = [\n",
    "#     {\"Mod√®le\": \"TF-IDF + LogReg\", \"Accuracy\": 0.85, \"F1-score\": 0.84, \"Recall\": 0.85, \"Taille (Mo)\": 1.2, \"Inf√©rence (ms)\": 0.8},\n",
    "#     {\"Mod√®le\": \"LSTM Baseline\", \"Accuracy\": 0.86, \"F1-score\": 0.85, \"Recall\": 0.86, \"Taille (Mo)\": 2.1, \"Inf√©rence (ms)\": 1.2},\n",
    "#     {\"Mod√®le\": \"Word2Vec\", \"Accuracy\": 0.87, \"F1-score\": 0.86, \"Recall\": 0.87, \"Taille (Mo)\": 3.5, \"Inf√©rence (ms)\": 1.5},\n",
    "#     {\"Mod√®le\": \"GloVe\", \"Accuracy\": 0.88, \"F1-score\": 0.87, \"Recall\": 0.88, \"Taille (Mo)\": 3.6, \"Inf√©rence (ms)\": 1.6},\n",
    "#     {\"Mod√®le\": \"FastText\", \"Accuracy\": 0.87, \"F1-score\": 0.86, \"Recall\": 0.87, \"Taille (Mo)\": 4.0, \"Inf√©rence (ms)\": 1.7},\n",
    "#     {\"Mod√®le\": \"BERT + LogisticRegression\", \"Accuracy\": 0.89, \"F1-score\": 0.88, \"Recall\": 0.89, \"Taille (Mo)\": 420.0, \"Inf√©rence (ms)\": 2.1}\n",
    "#     # {\"Mod√®le\": \"USE\", ...},   # Idem\n",
    "# ]\n",
    "\n",
    "# df_summary = pd.DataFrame(summary_data)\n",
    "# df_summary = df_summary.set_index(\"Mod√®le\")\n",
    "# display(df_summary.sort_values(by=\"Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14db3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üìä Synth√®se manuelle des performances √† compl√©ter apr√®s ex√©cution\n",
    "# import pandas as pd\n",
    "\n",
    "# summary_data = [\n",
    "#     {\"Mod√®le\": \"TF-IDF + LogReg\", \"Accuracy\": 0.85, \"F1-score\": 0.84, \"Recall\": 0.85, \"Taille (Mo)\": 1.2, \"Inf√©rence (ms)\": 0.8},\n",
    "#     {\"Mod√®le\": \"LSTM Baseline\", \"Accuracy\": 0.86, \"F1-score\": 0.85, \"Recall\": 0.86, \"Taille (Mo)\": 2.1, \"Inf√©rence (ms)\": 1.2},\n",
    "#     {\"Mod√®le\": \"Word2Vec\", \"Accuracy\": 0.87, \"F1-score\": 0.86, \"Recall\": 0.87, \"Taille (Mo)\": 3.5, \"Inf√©rence (ms)\": 1.5},\n",
    "#     {\"Mod√®le\": \"GloVe\", \"Accuracy\": 0.88, \"F1-score\": 0.87, \"Recall\": 0.88, \"Taille (Mo)\": 3.6, \"Inf√©rence (ms)\": 1.6},\n",
    "#     {\"Mod√®le\": \"FastText\", \"Accuracy\": 0.87, \"F1-score\": 0.86, \"Recall\": 0.87, \"Taille (Mo)\": 4.0, \"Inf√©rence (ms)\": 1.7},\n",
    "#     {\"Mod√®le\": \"BERT + LogisticRegression\", \"Accuracy\": 0.89, \"F1-score\": 0.88, \"Recall\": 0.89, \"Taille (Mo)\": 420.0, \"Inf√©rence (ms)\": 2.1}\n",
    "#     {\"Mod√®le\": \"LSTM + Word2Vec\", \"Accuracy\": 0.88, \"F1-score\": 0.87, \"Recall\": 0.88, \"Taille (Mo)\": 2.8, \"Inf√©rence (ms)\": 1.4},\n",
    "# {\"Mod√®le\": \"LSTM + FastText\", \"Accuracy\": 0.89, \"F1-score\": 0.88, \"Recall\": 0.89, \"Taille (Mo)\": 2.9, \"Inf√©rence (ms)\": 1.5},\n",
    "# {\"Mod√®le\": \"BERT + Dense\", \"Accuracy\": 0.90, \"F1-score\": 0.89, \"Recall\": 0.90, \"Taille (Mo)\": 3.5, \"Inf√©rence (ms)\": 2.3},\n",
    "# {\"Mod√®le\": \"TFBertModel + Dense\", \"Accuracy\": 0.91, \"F1-score\": 0.90, \"Recall\": 0.91, \"Taille (Mo)\": 4.2, \"Inf√©rence (ms)\": 2.5},\n",
    "# # {\"Mod√®le\": \"USE\", ...},   # Idem\n",
    "# ]\n",
    "\n",
    "# df_summary = pd.DataFrame(summary_data)\n",
    "# df_summary = df_summary.set_index(\"Mod√®le\")\n",
    "# display(df_summary.sort_values(by=\"Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544f890",
   "metadata": {},
   "source": [
    "## 9. Synth√®se comparative via MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d50caa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Size (MB)</th>\n",
       "      <th>Inference (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>LSTM Baseline</td>\n",
       "      <td>0.799087</td>\n",
       "      <td>0.799479</td>\n",
       "      <td>0.801042</td>\n",
       "      <td>15.058296</td>\n",
       "      <td>0.269115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>LSTM + FastText (local)</td>\n",
       "      <td>0.792200</td>\n",
       "      <td>0.798295</td>\n",
       "      <td>0.822417</td>\n",
       "      <td>129.677254</td>\n",
       "      <td>0.335439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>LSTM + Word2Vec</td>\n",
       "      <td>0.795937</td>\n",
       "      <td>0.797162</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>129.677254</td>\n",
       "      <td>0.305265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>LSTM Baseline</td>\n",
       "      <td>0.797650</td>\n",
       "      <td>0.794551</td>\n",
       "      <td>0.782567</td>\n",
       "      <td>15.058296</td>\n",
       "      <td>0.328464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>LSTM Baseline</td>\n",
       "      <td>0.796371</td>\n",
       "      <td>0.789321</td>\n",
       "      <td>0.762908</td>\n",
       "      <td>15.053310</td>\n",
       "      <td>0.290775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.726967</td>\n",
       "      <td>0.641742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM + Word2Vec</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.388390</td>\n",
       "      <td>0.861837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM + FastText (local)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.388390</td>\n",
       "      <td>0.980735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.726967</td>\n",
       "      <td>0.642071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy  F1-score    Recall   Size (MB)  \\\n",
       "75            LSTM Baseline  0.799087  0.799479  0.801042   15.058296   \n",
       "82  LSTM + FastText (local)  0.792200  0.798295  0.822417  129.677254   \n",
       "83          LSTM + Word2Vec  0.795937  0.797162  0.801975  129.677254   \n",
       "84            LSTM Baseline  0.797650  0.794551  0.782567   15.058296   \n",
       "92            LSTM Baseline  0.796371  0.789321  0.762908   15.053310   \n",
       "..                      ...       ...       ...       ...         ...   \n",
       "5                  Word2Vec  0.500000  0.000000  0.000000   14.726967   \n",
       "4           LSTM + Word2Vec  0.500000  0.000000  0.000000    1.388390   \n",
       "3   LSTM + FastText (local)  0.500000  0.000000  0.000000    1.388390   \n",
       "21                 Word2Vec  0.500000  0.000000  0.000000   14.726967   \n",
       "94                     None       NaN       NaN       NaN         NaN   \n",
       "\n",
       "    Inference (ms)  \n",
       "75        0.269115  \n",
       "82        0.335439  \n",
       "83        0.305265  \n",
       "84        0.328464  \n",
       "92        0.290775  \n",
       "..             ...  \n",
       "5         0.641742  \n",
       "4         0.861837  \n",
       "3         0.980735  \n",
       "21        0.642071  \n",
       "94             NaN  \n",
       "\n",
       "[95 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìä Synth√®se automatique via MLflow des mod√®les test√©s\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = mlflow.get_experiment_by_name(\"air_paradis_sentiment\")\n",
    "runs = client.search_runs(experiment_ids=[experiment.experiment_id], max_results=100)\n",
    "\n",
    "data = []\n",
    "for run in runs:\n",
    "    metrics = run.data.metrics\n",
    "    params = run.data.params\n",
    "    data.append({\n",
    "        \"Model\": params.get(\"model_type\"),\n",
    "        \"Accuracy\": metrics.get(\"accuracy\"),\n",
    "        \"F1-score\": metrics.get(\"f1_score\"),\n",
    "        \"Recall\": metrics.get(\"recall\"),\n",
    "        \"Size (MB)\": metrics.get(\"model_size_mb\"),\n",
    "        \"Inference (ms)\": metrics.get(\"inference_time_ms\")\n",
    "    })\n",
    "\n",
    "df_mlflow_summary = pd.DataFrame(data).sort_values(by=\"F1-score\", ascending=False)\n",
    "display(df_mlflow_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a26fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print (tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9edb541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.test.is_built_with_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f83be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîç V√©rification de l'installation CUDA et d√©tection GPU avec TensorFlow\n",
    "# import tensorflow as tf\n",
    "# print(\"üß† Version TensorFlow :\", tf.__version__)\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     print(f'üöÄ GPU d√©tect√© : {gpus}')\n",
    "# else:\n",
    "#     print('‚ùå Aucun GPU d√©tect√© par TensorFlow.')\n",
    "\n",
    "# # Affiche les informations CUDA/cuDNN si disponibles\n",
    "# try:\n",
    "#     from tensorflow.python.platform import build_info as tf_build_info\n",
    "#     print('CUDA version:', getattr(tf_build_info, 'cuda_version', 'N/A'))\n",
    "#     print('cuDNN version:', getattr(tf_build_info, 'cudnn_version', 'N/A'))\n",
    "# except ImportError:\n",
    "#     print('Impossible d\\'obtenir les versions CUDA/cuDNN via TensorFlow.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06edafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "# print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "# print(\"CUDA version:\", tf.sysconfig.get_build_info().get('cuda_version', 'N/A'))\n",
    "# print(\"cuDNN version:\", tf.sysconfig.get_build_info().get('cudnn_version', 'N/A'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
